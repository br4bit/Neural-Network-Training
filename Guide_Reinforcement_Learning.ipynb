{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Guide Reinforcement Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/br4bit/Neural-Network-Training/blob/master/Guide_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7y5M_RAVXbne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning (Apprendimento Rinforzato)\n",
        "\n",
        "Il *Reinforcement Learning (RL)* è uno dei campi più eccitanti del Machine Learning oggi, e anche uno dei più vecchi, esiste dagli anni '50.\n",
        "\n",
        "## Imparare ad ottimizzare i rewards.\n",
        "\n",
        "Nel Reinforcement Learning, un software *agent* effettua delle *osservazioni* e intraprende delle *azioni* all'interno di un *ambiente (environment)*  e in cambio riceve dei *rewards*.\n",
        "\n",
        "Il suo obiettivo è imparare ad agire in modo da massimizzare i suoi *rewards* a lungo termine. E' possibile pensare a *rewards* positivi come un piacere, e *rewards* negativi come un dolore/pena. In breve, l'agente agisce nell'ambiente e impara per tentativi ed errori a massimizzare il suo piacere e a minimizzare la sua pena.\n",
        "\n",
        "Vediamo alcuni esempi di applicazione:\n",
        "\n",
        "1 (a). L'agente può essere un programma che controlla il movimento di un robot. In questo caso, l'ambiente è il mondo reale, l'agente osserva l'ambiente attraverso un set di sensori, come camera e sensori touch, e agisce inviando segnali ai motori per attivarli. Può essere programmato per ricevere rewards positivi ogni volta che si avvicina alla destinazione, e reward negativi qualora perdesse tempo, andando nella direzione sbagliata o cade.\n",
        "\n",
        "2 (b). L'agente può essere un programma che controlla Ms. Pac-Man. In questo caso, l'ambiente è la simulazione del gioco Atari. le azioni da intraprendere sono le 9 possibili posizioni del joystick (sopra,sotto,e così via), le osservazioni da cui osservare per intraprendere le azioni saranno gli screenshots, mentre i rewards i game point.\n",
        "\n",
        "3 (c). L'agente può essere un semplice programma che gioca ad un gioco da tavolo come *Go*.\n",
        "\n",
        "4 (d). Non necessariamente l'agente deve controllare dei movimenti virtuali o reali di un qualcosa. Ad esempio, può essere un semplice termostato, che riceve dei rewards ogni volta che è vicino alla suo obiettivo di temperatura e risparmia energia e rewards negativi ogni volta che una mano esterna gli modifichi la temperatura, in questo modo l'agente impara ad anticipare i bisogni umani.\n",
        "\n",
        "5 (e). L'agente può osservare il prezzo di una merce in magazzino per decidere quando comprare e vendere ogni secondo. I rewards saranno ovviamente i guadagni e le perdite monetarie.\n",
        "\n",
        "![alt text](https://i.gyazo.com/thumb/1000/8e9980121e73e72575469bc49abaff56-png.jpg)\n",
        "\n",
        "\n",
        "Da notare che non sempre possono esserci rewards positivi; per sempio, l'agente può muoversi in un labirinto, ricevendo rewards negativi ad ogni istante di tempo, dove è meglio trovare un'uscita il più velocemente possibile. Ci sono un mucchio di task dove il RL è stato applicato come nel self-driving cars, per gli ads di una web page, o per controllare dove l'attenzione è focalizzata in un sistema di classificazione d'immagini."
      ]
    },
    {
      "metadata": {
        "id": "QX4yG267zmZY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Policy Search (Determinazone delle azioni da intraprendere)\n",
        "\n",
        "Il *Policy Search* può essere visto come una linea di condotta o un metodo per determinare le azioni che l'agente deve intraprendere. In breve, è un algoritmo usato dal software agent per determinare le sue azioni, è chiamato la sua *policy*. Per esempio, la policy potrebbe essere una rete neurale che riceve in input le osservazioni e da come output le azioni da compiere:\n",
        "\n",
        "![alt text](https://i.gyazo.com/d2553cf9477314cc2d9db6282b49562c.png)\n",
        "\n",
        "\n",
        "La policy può essere qualsiasi algoritmo a cui si possa pensare, e non deve essere nemmeno deterministico. Ad esempio, consideriamo un'aspirapolvere robotica, la cui ricompensa è la quantità di polvere che raccoglie in 30 minuti. La sua policy potrebbe essere quella di muoversi in avanti con una probabilità *p* ogni secondo, oppure ruotare in modo casuale a destra o a sinistra con una probabilità di *1-p*. L'angolo di rotazione sarà un angolo casuale compreso tra *-r e r*. Dal momento che questa policy introduce un pò di casualità è chiamata *stochastic policy*.\n",
        "Il robot avrà una traiettoria errata, che garantisce che alla fine raggiungerà qualsiasi posto possa raggiungere e raccoglierà tutta la polvere. La domanda è: Quanta polvere riuscirà a raccogliere in 30 minuti?\n",
        "\n",
        "Come si può addestrare questo robot? Ci sono solo due *policy parametri* che si possono modificare: la probabilità *p* e l'angolo *r*. Un possibile algoritmo di apprendimento potrebbe essere quello di provare molti valori diversi per questi parametri, e scegliere la combinazione che performa al meglio. Questo è un esempio di *policy search*, in questo caso utilizzando un approccio a forza bruta. Tuttavia quando lo *spazio di policy* è molto ampio, trovare un buon set di parametri in questo modo è come cercare un ago in un pagliaio.\n",
        "\n",
        "Un'altra strada per esplorare lo *spazio di policy (policy space)*  è di usare un *genetic algorithms (algoritmi genetici)*. Per esempio, si potrebbe creare casualmente una prima generazione di 100 policies (criteri) e provarli, uccidere le 80 peggiori policies, e lasciare le restanti 20 come sopravvisuti che producono 4 figli ciascuno. Un figlio non è altro che una copia del suo genitore più una variazione casuale. Le policies sopravvisuti più la loro prole (figli) insieme costituiranno la seconda generazione. Il processo si itera attraverso tutte le generazioni in questo modo, finchè non si trova la policy perfetta.\n",
        "\n",
        "![alt text](https://i.gyazo.com/ff957f4cf89c999079ebacbcb494c83c.png)\n",
        "\n",
        "\n",
        "Un altro approccio può essere quello di una tecnica di ottimizzazione, che valuta i gradienti di ogni reward rispetto ai parametri della policy, quindi aggiustando questi parametri seguendo il gradiente verso i rewards più alti *gradient ascent*. Questo approccio è chiamato *policy gradients (PG)*.\n",
        "Per esempio, torniamo indietro all'aspirapolvere, si potrebbe aumentare leggermente *p* e valutare se questo aumenta la quantità di polvere raccolta dal robot in 30 minuti; se lo fa, allora incrementa *p* un pò di più, o altrimenti riduci *p*. \n",
        "\n",
        "Prima di implementare l'algoritmo di PG usando TensorFlow, c'è bisogno di creare un ambiente per l'agente dove possa vivere. Introduciamo OpenAI gym."
      ]
    },
    {
      "metadata": {
        "id": "qYZQv-Fm9Jz7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduzione OpenAI gym\n",
        "\n",
        "Una delle sfide dell'apprendimento rinforzato è che per addestare un agente c'è bisogno di un ambiente di lavoro. Se si vuole programmare un agente che impara a giocare ad un gioco Atari, c'è bisogno di un simulatore di gioco Atari. Se si vuole programmare un robot che cammini, allora l'ambiente è il mondo reale dove si può addestrare direttamente il robot in quel ambiente, ma con delle limitazioni: se il robot cade da una scogliera, non basta solo \"annullare\" l'azione. Non è possibile accelerare il tempo; aggiungere più potenza computazionale non farà si che il robot si muova più velocemente. E in genere è molto costoso addestrare 1000 robot in parallelo.\n",
        "\n",
        "In breve, l'addestramento nel mondo reale è lento e difficile, quindi c'è bisogno di un simulatore di ambienti, almeno per la prima fase dell'addestramento (bootstrap training).\n",
        "\n",
        "*OpenAI gym* è un kit di strumenti che forniscono una varietà di simulatori di ambienti (Atari, tavola degli scacchi, mondo 2D, 3D, ecc), così si possono addestrare agenti, compararli tra di loro e implementare nuovi algoritmi di RL.\n",
        "\n",
        "Installazione di *OpenAI gym*:"
      ]
    },
    {
      "metadata": {
        "id": "2wTFk6WCT20x",
        "colab_type": "code",
        "outputId": "bb0a3161-ff2f-49d7-ed83-cf58ca0b4528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gym in /usr/local/lib/python3.6/dist-packages (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QN3c2HmRIRXN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creazione del primo ambiente:"
      ]
    },
    {
      "metadata": {
        "id": "GmKAr3woLJ2W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In google colab per utilizzare la funzione env.render() c'è bisogno di un piccolo passaggio:"
      ]
    },
    {
      "metadata": {
        "id": "C05DB6fSLRnF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OcGkLYYLddT",
        "colab_type": "code",
        "outputId": "2e893196-899d-489b-8dd1-b6965830e5be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#eg screen resolution 1400x900\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "2QjXbY2ILren",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gpAtOTnpIKhW",
        "colab_type": "code",
        "outputId": "57e686a3-8760-452e-ea42-13edf0bebd56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "env = gym.make(\"CartPole-v0\") #creazione ambiente\n",
        "\n",
        "obs = env.reset() #osservazioni\n",
        "\n",
        "obs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.00389311,  0.03207835, -0.03010357, -0.00629941])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "D2yCq6U4OL0Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "La funzione *make()* crea l'ambiente, in questo caso il carrello con asta. E' una simulazione 2D dove il carrello può accelerare nella direazione di destra e sinistra, per bilanciare l'asta posta al di sopra di esso:\n",
        "\n",
        "![alt text](https://i.gyazo.com/0a9d7b15a9877b4ab5351ac643414084.png)\n",
        "\n",
        "Dopo che l'ambiente è stato creato, bisogna inizializzarlo usando il metodo *reset()*. Il metodo ritorna la prima osservazione. Le osservazioni dipendono dal tipo di ambiente. Per il carrello con asta, ogni osservazione è un NumPy array di 1D, contenente 4 valori di tipo float: questi valori rappresentano, la posizione del carrello in orizzontale (0.0 = centro), la sua velocità, l'angolo dell'asta (0.0 = verticale) e la sua velocità angolare.\n",
        "\n",
        "Chiediamo ora all'ambiente quali sono le possibili azioni con il metodo *action_space*:"
      ]
    },
    {
      "metadata": {
        "id": "O6b3U9PhIfcg",
        "colab_type": "code",
        "outputId": "1a183f71-1fe8-4ef9-efd1-16757cb5d7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "W4tF4ytzRgyC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Discrete(2)* significa che le possibili azioni sono interi 0 e 1, che rappresentano l'accelerazione vero destra (1) e verso sinistra (0). Altri ambienti avranno più azioni o un altro tipo di azioni (nel continuo).\n",
        "Dato che l'asta è inclinata verso destra, acceleriamo il carrello verso destra:"
      ]
    },
    {
      "metadata": {
        "id": "2orD8WzNQBSD",
        "colab_type": "code",
        "outputId": "b37d16b4-c30e-4566-ce10-3f61de23d6c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "action = 1 #accelerazione verso destra\n",
        "\n",
        "obs, reward, done, info = env.step(action)\n",
        "\n",
        "print(obs,reward,done,info)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.00325155  0.2276188  -0.03022956 -0.30832632] 1.0 False {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iTlUTkgAWFLx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Il metodo *step()* esegue l'azione sull'ambiente e ritorna 4 valori:\n",
        "\n",
        "**obs** \n",
        "\n",
        ">Questa è la nuova osservazione. Il carrello ora si muove verso destra **(obs[1]>0)**. L'asta è inclinata verso sinistra **(obs[2]<0)**, e la sua velocità angolare è negativa **(obs[3]<0)**, sarà probabilmente inclinata verso sinista al prossimo step. \n",
        "\n",
        "**reward**\n",
        "\n",
        ">In questo ambiente, si avrà sempre un reward di 1.0 ad ogni step, non importa cosa fai, l'obiettivo è quello di continuare a muovere il carrello il più a lungo possibile.\n",
        "\n",
        "**done**\n",
        "\n",
        ">Questo valore sarà *True* quando *episodio* è finito. Questo accadrà quando l'asta si inclinerà troppo. Dopodichè l'ambiente dovrà essere ripristinato prima di poterlo riutilizzare dinuovo.\n",
        "\n",
        "**info**\n",
        "\n",
        ">Questo dizionario può fornire ulteriori informazioni di debug in altri ambienti. Questi dati non dovrebbero essere usati per l'addestramento (usarli equivale a barare).\n",
        "\n",
        "\n",
        "Proviamo a scrivere un pezzo di codice per una semplice policy che accelera il carrello verso destra quando l'asta è inclinata verso destra e accelera verso sinistra quando l'asta è inclinata a sinistra. Eseguiremo questa policy su 500 episodi per vedere i rewards medi che ottiene:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SlnF4GKnUifJ",
        "colab_type": "code",
        "outputId": "b1c0c9d3-42c4-40be-dfd6-57408166a775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "def azione_base(obs):\n",
        "  angle = obs[2]\n",
        "  return 0 if angle < 0 else 1\n",
        "\n",
        "totals = []\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array')) # Immagine ambiente\n",
        "\n",
        "for episode in range(500):\n",
        "  episode_rewards = 0 #Ad ogni inizio dell' episodio il reward è inizilizzato a 0\n",
        "                      #questo perchè ad ogni uscita dal ciclo degli step, \n",
        "                      #l'asta si è inclinata troppo (done=true) o sono terminati gli step\n",
        "  obs = env.reset()\n",
        "  for step in range(1000): # Un massimo di 1000 step\n",
        "    action = azione_base(obs)\n",
        "    obs,reward,done,info = env.step(action)\n",
        "    episode_rewards += reward #contatore dei rewards\n",
        "    #Immagini del carrello con asta ad ogni step\n",
        "      #img.set_data(env.render(mode='rgb_array')) # just update the data\n",
        "      #display.display(plt.gcf())\n",
        "      #display.clear_output(wait=True)\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "  totals.append(episode_rewards) #contiene il numero di rewards di ogni episode.\n",
        "  \n",
        "print(len(totals),totals[:3]) #numero di elementi e i primi 3 elementi della lista"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 [34.0, 26.0, 49.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD8CAYAAACINTRsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7D2Luks3LVB5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Andiamo ora a vedere la media dei rewards positivi e altre informazioni:"
      ]
    },
    {
      "metadata": {
        "id": "MdwEToGqI1nA",
        "colab_type": "code",
        "outputId": "24c6dd32-b9cb-44fe-af95-20f92c6c7b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42.092, 8.764219075308421, 24.0, 68.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "SCMOYQWDP-Fr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Anche con 500 prove, questa policy non riesce a mantenere l'asta in verticale per più di 68 step consecutivi. Non molto bene. Dal render dell'ambiente è possibile vedere che il carro oscilla a destra e a sinistra finchè l'asta non si inclina del tutto.\n",
        "\n",
        "Vediamo se una rete neurale può elaborare una policy migliore."
      ]
    },
    {
      "metadata": {
        "id": "KTY2ia18QsH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Network Policies\n",
        "\n",
        "Creiamo una policy con rete neurale. Come la policy programmata sopra, anche questa rete neurale avrà come input un'osservazione e darà come output un' azione da eseguire. Più precisamente si stimerà la probabilità di ogni azione, e quindi selezioneremo un'azione in modo casuale in base alle probabilità stimate. Nell'ambiente del carrello con asta, ci sono solo due possibili azioni (destra o sinistra), c'è bisogno di un solo neurone di output. L'output sarà la probabilità *p* dell'azione 0 (sinistra), e la probabilità dell'azione 1 (destra) sarà *1-p*. Ad esempio, se l'output della rete sarà 0.7, allora si utilizzerà l'azione 0 con il 70% della probabilità, e l'azione 1 con il 30% di probabilità. \n",
        "\n",
        "![alt text](https://i.gyazo.com/2e55e2774da74940c4861290438e351b.png)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YDR2Qg9BP5r8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}