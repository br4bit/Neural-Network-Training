{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Guide Reinforcement Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/br4bit/Neural-Network-Training/blob/master/Guide_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7y5M_RAVXbne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning (Apprendimento Rafforzativo)\n",
        "\n",
        "Il *Reinforcement Learning (RL)* è uno dei campi più eccitanti del Machine Learning oggi, e anche uno dei più vecchi, esiste dagli anni '50.\n",
        "\n",
        "## Imparare ad ottimizzare i rewards.\n",
        "\n",
        "Nel Reinforcement Learning, un software *agent* effettua delle *osservazioni* e intraprende delle *azioni* all'interno di un *ambiente (environment)*  e in cambio riceve dei *rewards*.\n",
        "\n",
        "Il suo obiettivo è imparare ad agire in modo da massimizzare i suoi *rewards* a lungo termine. E' possibile pensare a *rewards* positivi come un piacere, e *rewards* negativi come un dolore/pena. In breve, l'agente agisce nell'ambiente e impara per tentativi ed errori a massimizzare il suo piacere e a minimizzare la sua pena.\n",
        "\n",
        "Vediamo alcuni esempi di applicazione:\n",
        "\n",
        "1 (a). L'agente può essere un programma che controlla il movimento di un robot. In questo caso, l'ambiente è il mondo reale, l'agente osserva l'ambiente attraverso un set di sensori, come camera e sensori touch, e agisce inviando segnali ai motori per attivarli. Può essere programmato per ricevere rewards positivi ogni volta che si avvicina alla destinazione, e reward negativi qualora perdesse tempo, andando nella direzione sbagliata o cade.\n",
        "\n",
        "2 (b). L'agente può essere un programma che controlla Ms. Pac-Man. In questo caso, l'ambiente è la simulazione del gioco Atari. le azioni da intraprendere sono le 9 possibili posizioni del joystick (sopra,sotto,e così via), le osservazioni da cui osservare per intraprendere le azioni saranno gli screenshots, mentre i rewards i game point.\n",
        "\n",
        "3 (c). L'agente può essere un semplice programma che gioca ad un gioco da tavolo come *Go*.\n",
        "\n",
        "4 (d). Non necessariamente l'agente deve controllare dei movimenti virtuali o reali di un qualcosa. Ad esempio, può essere un semplice termostato, che riceve dei rewards ogni volta che è vicino alla suo obiettivo di temperatura e risparmia energia e rewards negativi ogni volta che una mano esterna gli modifichi la temperatura, in questo modo l'agente impara ad anticipare i bisogni umani.\n",
        "\n",
        "5 (e). L'agente può osservare il prezzo di una merce in magazzino per decidere quando comprare e vendere ogni secondo. I rewards saranno ovviamente i guadagni e le perdite monetarie.\n",
        "\n",
        "![alt text](https://i.gyazo.com/thumb/1000/8e9980121e73e72575469bc49abaff56-png.jpg)\n",
        "\n",
        "\n",
        "Da notare che non sempre possono esserci rewards positivi; per sempio, l'agente può muoversi in un labirinto, ricevendo rewards negativi ad ogni istante di tempo, dove è meglio trovare un'uscita il più velocemente possibile. Ci sono un mucchio di task dove il RL è stato applicato come nel self-driving cars, per gli ads di una web page, o per controllare dove l'attenzione è focalizzata in un sistema di classificazione d'immagini."
      ]
    },
    {
      "metadata": {
        "id": "QX4yG267zmZY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Policy Search (Determinazone delle azioni da intraprendere)\n",
        "\n",
        "Il *Policy Search* può essere visto come una linea di condotta o un metodo per determinare le azioni che l'agente deve intraprendere. In breve, è un algoritmo usato dal software agent per determinare le sue azioni, è chiamato la sua *policy*. Per esempio, la policy potrebbe essere una rete neurale che riceve in input le osservazioni e da come output le azioni da compiere:\n",
        "\n",
        "![alt text](https://i.gyazo.com/d2553cf9477314cc2d9db6282b49562c.png)\n",
        "\n",
        "\n",
        "La policy può essere qualsiasi algoritmo a cui si possa pensare, e non deve essere nemmeno deterministico. Ad esempio, consideriamo un'aspirapolvere robotica, la cui ricompensa è la quantità di polvere che raccoglie in 30 minuti. La sua policy potrebbe essere quella di muoversi in avanti con una probabilità *p* ogni secondo, oppure ruotare in modo casuale a destra o a sinistra con una probabilità di *1-p*. L'angolo di rotazione sarà un angolo casuale compreso tra *-r e r*. Dal momento che questa policy introduce un pò di casualità è chiamata *stochastic policy*.\n",
        "Il robot avrà una traiettoria errata, che garantisce che alla fine raggiungerà qualsiasi posto possa raggiungere e raccoglierà tutta la polvere. La domanda è: Quanta polvere riuscirà a raccogliere in 30 minuti?\n",
        "\n",
        "Come si può addestrare questo robot? Ci sono solo due *policy parametri* che si possono modificare: la probabilità *p* e l'angolo *r*. Un possibile algoritmo di apprendimento potrebbe essere quello di provare molti valori diversi per questi parametri, e scegliere la combinazione che performa al meglio. Questo è un esempio di *policy search*, in questo caso utilizzando un approccio a forza bruta. Tuttavia quando lo *spazio di policy* è molto ampio, trovare un buon set di parametri in questo modo è come cercare un ago in un pagliaio.\n",
        "\n",
        "Un'altra strada per esplorare lo *spazio di policy (policy space)*  è di usare un *genetic algorithms (algoritmi genetici)*. Per esempio, si potrebbe creare casualmente una prima generazione di 100 policies (criteri) e provarli, uccidere le 80 peggiori policies, e lasciare le restanti 20 come sopravvisuti che producono 4 figli ciascuno. Un figlio non è altro che una copia del suo genitore più una variazione casuale. Le policies sopravvisuti più la loro prole (figli) insieme costituiranno la seconda generazione. Il processo si itera attraverso tutte le generazioni in questo modo, finchè non si trova la policy perfetta.\n",
        "\n",
        "![alt text](https://i.gyazo.com/ff957f4cf89c999079ebacbcb494c83c.png)\n",
        "\n",
        "\n",
        "Un altro approccio può essere quello di una tecnica di ottimizzazione, che valuta i gradienti di ogni reward rispetto ai parametri della policy, quindi aggiustando questi parametri seguendo il gradiente verso i rewards più alti *gradient ascent*. Questo approccio è chiamato *policy gradients (PG)*.\n",
        "Per esempio, torniamo indietro all'aspirapolvere, si potrebbe aumentare leggermente *p* e valutare se questo aumenta la quantità di polvere raccolta dal robot in 30 minuti; se lo fa, allora incrementa *p* un pò di più, o altrimenti riduci *p*. \n",
        "\n",
        "Prima di implementare l'algoritmo di PG usando TensorFlow, c'è bisogno di creare un ambiente per l'agente dove possa vivere. Introduciamo OpenAI gym."
      ]
    },
    {
      "metadata": {
        "id": "qYZQv-Fm9Jz7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "2wTFk6WCT20x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}