{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Guide Reinforcement Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/br4bit/Neural-Network-Training/blob/master/Guide_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7y5M_RAVXbne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning (Apprendimento Rinforzato)\n",
        "\n",
        "Il *Reinforcement Learning (RL)* è uno dei campi più eccitanti del Machine Learning oggi, e anche uno dei più vecchi, esiste dagli anni '50.\n",
        "\n",
        "## Imparare ad ottimizzare i rewards.\n",
        "\n",
        "Nel Reinforcement Learning, un software *agent* effettua delle *osservazioni* e intraprende delle *azioni* all'interno di un *ambiente (environment)*  e in cambio riceve dei *rewards*.\n",
        "\n",
        "Il suo obiettivo è imparare ad agire in modo da massimizzare i suoi *rewards* a lungo termine. E' possibile pensare a *rewards* positivi come un piacere, e *rewards* negativi come un dolore/pena. In breve, l'agente agisce nell'ambiente e impara per tentativi ed errori a massimizzare il suo piacere e a minimizzare la sua pena.\n",
        "\n",
        "Vediamo alcuni esempi di applicazione:\n",
        "\n",
        "1 (a). L'agente può essere un programma che controlla il movimento di un robot. In questo caso, l'ambiente è il mondo reale, l'agente osserva l'ambiente attraverso un set di sensori, come camera e sensori touch, e agisce inviando segnali ai motori per attivarli. Può essere programmato per ricevere rewards positivi ogni volta che si avvicina alla destinazione, e reward negativi qualora perdesse tempo, andando nella direzione sbagliata o cade.\n",
        "\n",
        "2 (b). L'agente può essere un programma che controlla Ms. Pac-Man. In questo caso, l'ambiente è la simulazione del gioco Atari. le azioni da intraprendere sono le 9 possibili posizioni del joystick (sopra,sotto,e così via), le osservazioni da cui osservare per intraprendere le azioni saranno gli screenshots, mentre i rewards i game point.\n",
        "\n",
        "3 (c). L'agente può essere un semplice programma che gioca ad un gioco da tavolo come *Go*.\n",
        "\n",
        "4 (d). Non necessariamente l'agente deve controllare dei movimenti virtuali o reali di un qualcosa. Ad esempio, può essere un semplice termostato, che riceve dei rewards ogni volta che è vicino alla suo obiettivo di temperatura e risparmia energia e rewards negativi ogni volta che una mano esterna gli modifichi la temperatura, in questo modo l'agente impara ad anticipare i bisogni umani.\n",
        "\n",
        "5 (e). L'agente può osservare il prezzo di una merce in magazzino per decidere quando comprare e vendere ogni secondo. I rewards saranno ovviamente i guadagni e le perdite monetarie.\n",
        "\n",
        "![alt text](https://i.gyazo.com/thumb/1000/8e9980121e73e72575469bc49abaff56-png.jpg)\n",
        "\n",
        "\n",
        "Da notare che non sempre possono esserci rewards positivi; per sempio, l'agente può muoversi in un labirinto, ricevendo rewards negativi ad ogni istante di tempo, dove è meglio trovare un'uscita il più velocemente possibile. Ci sono un mucchio di task dove il RL è stato applicato come nel self-driving cars, per gli ads di una web page, o per controllare dove l'attenzione è focalizzata in un sistema di classificazione d'immagini."
      ]
    },
    {
      "metadata": {
        "id": "QX4yG267zmZY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Policy Search (Determinazone delle azioni da intraprendere)\n",
        "\n",
        "Il *Policy Search* può essere visto come una linea di condotta o un metodo per determinare le azioni che l'agente deve intraprendere. In breve, è un algoritmo usato dal software agent per determinare le sue azioni, è chiamato la sua *policy*. Per esempio, la policy potrebbe essere una rete neurale che riceve in input le osservazioni e da come output le azioni da compiere:\n",
        "\n",
        "![alt text](https://i.gyazo.com/d2553cf9477314cc2d9db6282b49562c.png)\n",
        "\n",
        "\n",
        "La policy può essere qualsiasi algoritmo a cui si possa pensare, e non deve essere nemmeno deterministico. Ad esempio, consideriamo un'aspirapolvere robotica, la cui ricompensa è la quantità di polvere che raccoglie in 30 minuti. La sua policy potrebbe essere quella di muoversi in avanti con una probabilità *p* ogni secondo, oppure ruotare in modo casuale a destra o a sinistra con una probabilità di *1-p*. L'angolo di rotazione sarà un angolo casuale compreso tra *-r e r*. Dal momento che questa policy introduce un pò di casualità è chiamata *stochastic policy*.\n",
        "Il robot avrà una traiettoria errata, che garantisce che alla fine raggiungerà qualsiasi posto possa raggiungere e raccoglierà tutta la polvere. La domanda è: Quanta polvere riuscirà a raccogliere in 30 minuti?\n",
        "\n",
        "Come si può addestrare questo robot? Ci sono solo due *policy parametri* che si possono modificare: la probabilità *p* e l'angolo *r*. Un possibile algoritmo di apprendimento potrebbe essere quello di provare molti valori diversi per questi parametri, e scegliere la combinazione che performa al meglio. Questo è un esempio di *policy search*, in questo caso utilizzando un approccio a forza bruta. Tuttavia quando lo *spazio di policy* è molto ampio, trovare un buon set di parametri in questo modo è come cercare un ago in un pagliaio.\n",
        "\n",
        "Un'altra strada per esplorare lo *spazio di policy (policy space)*  è di usare un *genetic algorithms (algoritmi genetici)*. Per esempio, si potrebbe creare casualmente una prima generazione di 100 policies (criteri) e provarli, uccidere le 80 peggiori policies, e lasciare le restanti 20 come sopravvisuti che producono 4 figli ciascuno. Un figlio non è altro che una copia del suo genitore più una variazione casuale. Le policies sopravvisuti più la loro prole (figli) insieme costituiranno la seconda generazione. Il processo si itera attraverso tutte le generazioni in questo modo, finchè non si trova la policy perfetta.\n",
        "\n",
        "![alt text](https://i.gyazo.com/ff957f4cf89c999079ebacbcb494c83c.png)\n",
        "\n",
        "\n",
        "Un altro approccio può essere quello di una tecnica di ottimizzazione, che valuta i gradienti di ogni reward rispetto ai parametri della policy, quindi aggiustando questi parametri seguendo il gradiente verso i rewards più alti *gradient ascent*. Questo approccio è chiamato *policy gradients (PG)*.\n",
        "Per esempio, torniamo indietro all'aspirapolvere, si potrebbe aumentare leggermente *p* e valutare se questo aumenta la quantità di polvere raccolta dal robot in 30 minuti; se lo fa, allora incrementa *p* un pò di più, o altrimenti riduci *p*. \n",
        "\n",
        "Prima di implementare l'algoritmo di PG usando TensorFlow, c'è bisogno di creare un ambiente per l'agente dove possa vivere. Introduciamo OpenAI gym."
      ]
    },
    {
      "metadata": {
        "id": "qYZQv-Fm9Jz7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduzione OpenAI gym\n",
        "\n",
        "Una delle sfide dell'apprendimento rinforzato è che per addestare un agente c'è bisogno di un ambiente di lavoro. Se si vuole programmare un agente che impara a giocare ad un gioco Atari, c'è bisogno di un simulatore di gioco Atari. Se si vuole programmare un robot che cammini, allora l'ambiente è il mondo reale dove si può addestrare direttamente il robot in quel ambiente, ma con delle limitazioni: se il robot cade da una scogliera, non basta solo \"annullare\" l'azione. Non è possibile accelerare il tempo; aggiungere più potenza computazionale non farà si che il robot si muova più velocemente. E in genere è molto costoso addestrare 1000 robot in parallelo.\n",
        "\n",
        "In breve, l'addestramento nel mondo reale è lento e difficile, quindi c'è bisogno di un simulatore di ambienti, almeno per la prima fase dell'addestramento (bootstrap training).\n",
        "\n",
        "*OpenAI gym* è un kit di strumenti che forniscono una varietà di simulatori di ambienti (Atari, tavola degli scacchi, mondo 2D, 3D, ecc), così si possono addestrare agenti, compararli tra di loro e implementare nuovi algoritmi di RL.\n",
        "\n",
        "Installazione di *OpenAI gym*:"
      ]
    },
    {
      "metadata": {
        "id": "2wTFk6WCT20x",
        "colab_type": "code",
        "outputId": "9b4712f8-ef4b-40d0-c753-1dfce7ab5911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/98/fad8b85827d9aca922eec6a8747dd696aac281ebdc4b1225acf51f8f612f/gym-0.12.0.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 15.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/bf/b9/fd/e1f54a4b99eca10de0d37cc7ecc2ac29a2e51b4eff7e1e5ca7\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Found existing installation: gym 0.10.11\n",
            "    Uninstalling gym-0.10.11:\n",
            "      Successfully uninstalled gym-0.10.11\n",
            "Successfully installed gym-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QN3c2HmRIRXN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creazione del primo ambiente:"
      ]
    },
    {
      "metadata": {
        "id": "GmKAr3woLJ2W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In google colab per utilizzare la funzione env.render() c'è bisogno di un piccolo passaggio:"
      ]
    },
    {
      "metadata": {
        "id": "C05DB6fSLRnF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OcGkLYYLddT",
        "colab_type": "code",
        "outputId": "3e87ddce-daab-46db-e0d5-582c4c71efd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#eg screen resolution 1400x900\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "2QjXbY2ILren",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gpAtOTnpIKhW",
        "colab_type": "code",
        "outputId": "4af2094f-b152-4ad0-c34e-0b87d976d191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "env = gym.make(\"CartPole-v0\") #creazione ambiente\n",
        "\n",
        "obs = env.reset() #osservazioni\n",
        "\n",
        "obs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.02362206, 0.00019816, 0.01505991, 0.04627168])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "D2yCq6U4OL0Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "La funzione *make()* crea l'ambiente, in questo caso il carrello con asta. E' una simulazione 2D dove il carrello può accelerare nella direazione di destra e sinistra, per bilanciare l'asta posta al di sopra di esso:\n",
        "\n",
        "![alt text](https://i.gyazo.com/0a9d7b15a9877b4ab5351ac643414084.png)\n",
        "\n",
        "Dopo che l'ambiente è stato creato, bisogna inizializzarlo usando il metodo *reset()*. Il metodo ritorna la prima osservazione. Le osservazioni dipendono dal tipo di ambiente. Per il carrello con asta, ogni osservazione è un NumPy array di 1D, contenente 4 valori di tipo float: questi valori rappresentano, la posizione del carrello in orizzontale (0.0 = centro), la sua velocità, l'angolo dell'asta (0.0 = verticale) e la sua velocità angolare.\n",
        "\n",
        "Chiediamo ora all'ambiente quali sono le possibili azioni con il metodo *action_space*:"
      ]
    },
    {
      "metadata": {
        "id": "O6b3U9PhIfcg",
        "colab_type": "code",
        "outputId": "d22604f8-9d81-4b64-a6df-591eb2fb03d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "W4tF4ytzRgyC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Discrete(2)* significa che le possibili azioni sono interi 0 e 1, che rappresentano l'accelerazione vero destra (1) e verso sinistra (0). Altri ambienti avranno più azioni o un altro tipo di azioni (nel continuo).\n",
        "Dato che l'asta è inclinata verso destra, acceleriamo il carrello verso destra:"
      ]
    },
    {
      "metadata": {
        "id": "2orD8WzNQBSD",
        "colab_type": "code",
        "outputId": "90558aa2-4b00-4c1b-c178-ea819d3521bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "action = 1 #accelerazione verso destra\n",
        "\n",
        "obs, reward, done, info = env.step(action)\n",
        "\n",
        "print(obs,reward,done,info)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.02362602  0.19510096  0.01598534 -0.24162192] 1.0 False {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iTlUTkgAWFLx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Il metodo *step()* esegue l'azione sull'ambiente e ritorna 4 valori:\n",
        "\n",
        "**obs** \n",
        "\n",
        ">Questa è la nuova osservazione. Il carrello ora si muove verso destra **(obs[1]>0)**. L'asta è inclinata verso sinistra **(obs[2]<0)**, e la sua velocità angolare è negativa **(obs[3]<0)**, sarà probabilmente inclinata verso sinista al prossimo step. \n",
        "\n",
        "**reward**\n",
        "\n",
        ">In questo ambiente, si avrà sempre un reward di 1.0 ad ogni step, non importa cosa fai, l'obiettivo è quello di continuare a muovere il carrello il più a lungo possibile.\n",
        "\n",
        "**done**\n",
        "\n",
        ">Questo valore sarà *True* quando *episodio* è finito. Questo accadrà quando l'asta si inclinerà troppo. Dopodichè l'ambiente dovrà essere ripristinato prima di poterlo riutilizzare dinuovo.\n",
        "\n",
        "**info**\n",
        "\n",
        ">Questo dizionario può fornire ulteriori informazioni di debug in altri ambienti. Questi dati non dovrebbero essere usati per l'addestramento (usarli equivale a barare).\n",
        "\n",
        "\n",
        "Proviamo a scrivere un pezzo di codice per una semplice policy che accelera il carrello verso destra quando l'asta è inclinata verso destra e accelera verso sinistra quando l'asta è inclinata a sinistra. Eseguiremo questa policy su 500 episodi per vedere i rewards medi che ottiene:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SlnF4GKnUifJ",
        "colab_type": "code",
        "outputId": "337d5d1e-7054-42c0-9601-4accc7a15cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "def azione_base(obs):\n",
        "  angle = obs[2]\n",
        "  return 0 if angle < 0 else 1\n",
        "\n",
        "totals = []\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array')) # Immagine ambiente\n",
        "\n",
        "for episode in range(500):\n",
        "  episode_rewards = 0 #Ad ogni inizio dell' episodio il reward è inizilizzato a 0\n",
        "                      #questo perchè ad ogni uscita dal ciclo degli step, \n",
        "                      #l'asta si è inclinata troppo (done=true) o sono terminati gli step\n",
        "  obs = env.reset()\n",
        "  for step in range(1000): # Un massimo di 1000 step\n",
        "    action = azione_base(obs)\n",
        "    obs,reward,done,info = env.step(action)\n",
        "    episode_rewards += reward #contatore dei rewards\n",
        "    #Immagini del carrello con asta ad ogni step\n",
        "      #img.set_data(env.render(mode='rgb_array')) # just update the data\n",
        "      #display.display(plt.gcf())\n",
        "      #display.clear_output(wait=True)\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "  totals.append(episode_rewards) #contiene il numero di rewards di ogni episode.\n",
        "  \n",
        "print(len(totals),totals[:3]) #numero di elementi e i primi 3 elementi della lista"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 [54.0, 25.0, 52.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD8CAYAAACINTRsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKoihLkkFMdQlqSD/B2x1g989d39aAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7D2Luks3LVB5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Andiamo ora a vedere la media dei rewards positivi e altre informazioni:"
      ]
    },
    {
      "metadata": {
        "id": "MdwEToGqI1nA",
        "colab_type": "code",
        "outputId": "312b671e-db8f-4a02-ccf3-a1d722d92df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42.654, 8.841396043612118, 24.0, 71.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "SCMOYQWDP-Fr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Anche con 500 prove, questa policy non riesce a mantenere l'asta in verticale per più di 68 step consecutivi. Non molto bene. Dal render dell'ambiente è possibile vedere che il carro oscilla a destra e a sinistra finchè l'asta non si inclina del tutto.\n",
        "\n",
        "Il codice sopra è una classica implementazione del ciclo \"agente-ambiente\".Ad ogni step, l'agente sceglie un *action*, e l'ambiente ritorna un *observation* e un *reward*:\n",
        "\n",
        "![alt text](https://i.gyazo.com/390e7cd7e081ab42bab0650c1129069c.png)\n",
        "\n",
        "\n",
        "Vediamo se una rete neurale può elaborare una policy migliore."
      ]
    },
    {
      "metadata": {
        "id": "KTY2ia18QsH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Network Policies\n",
        "\n",
        "Creiamo una policy con rete neurale. Come la policy programmata sopra, anche questa rete neurale avrà come input un'osservazione e darà come output un' azione da eseguire. Più precisamente si stimerà la probabilità di ogni azione, e quindi selezioneremo un'azione in modo casuale in base alle probabilità stimate. Nell'ambiente del carrello con asta, ci sono solo due possibili azioni (destra o sinistra), c'è bisogno di un solo neurone di output. L'output sarà la probabilità *p* dell'azione 0 (sinistra), e la probabilità dell'azione 1 (destra) sarà *1-p*. Ad esempio, se l'output della rete sarà 0.7, allora si utilizzerà l'azione 0 con il 70% della probabilità, e l'azione 1 con il 30% di probabilità. \n",
        "\n",
        "![alt text](https://i.gyazo.com/2e55e2774da74940c4861290438e351b.png)\n",
        "\n",
        "Perchè prendere delle azioni casuali basate sulla probabilità ? Non risulta più pratico prendere l'azione che ha ottenuto lo score più alto dall'output ? Beh no!!!\n",
        "Questo approccio fa si che l'agente trovi il giusto bilanciamento tra l'esplorazione di nuove azioni e lo sfruttamento delle azioni che sono note per funzionare bene.\n",
        "\n",
        "Ecco un'analogia per chiarire il concetto: Supponiamo di essere in un ristorante per la prima volta, tutti i piatti sembrano appetitosi, quindi ne scegliamo uno a caso. Se il piatto è buono, allora aumenta la probabilità di riordinarlo la prossima volta, ma non dovresti aumentare questa probabilità del 100% o meglio scegliere sempre lo stesso piatto o altrimenti si preclude la possibilità di provarne altri, alcuni dei quali potrebbero essere anche migliori di quello provato.\n",
        "\n",
        "Da notare anche che questo tipo di ambiente le azioni passate e le osservazioni possono essere ignorate, poichè ogni azione contiene lo stato completo dell'ambiente. Se ci fosse qualche stato nascosto, allora potrebbe essere necessario considerare anche le azioni e le osservazioni pssate. Ad esempio, se l'ambiente rivela solamente la posizione del carrello ma non la sua velocità (tipico esempio di ambiente nel mondo reale), si dovrebbe considerare non solo l'osservazione corrente ma anche quella precedente in modo da stimare la sua velocità attuale.\n",
        "Un altro esempio è quando le osservazioni sono rumorose; in tal caso, si utilizzano le ultime osservazioni per stimare lo stato corrente più probabile.\n",
        "\n",
        "Il problema del carrello con asta è più semplice di come dovrebbe essere; le osservazioni sono senza rumore e contengono lo stato completo dell'ambiente.\n",
        "\n",
        "Utilizziamo TensorFlow per costruire la nostra policy con una rete neurale:\n"
      ]
    },
    {
      "metadata": {
        "id": "YDR2Qg9BP5r8",
        "colab_type": "code",
        "outputId": "ef0ba3b4-4b9e-4d19-c346-a7662657c93f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Specifica dell'architettura della rete neurale\n",
        "\n",
        "n_inputs = 4 # è la dimensione del vettore delle obs env.observation_space.shape[0]\n",
        "n_hidden = 4 # è un task semplice non c'è bisogno di più neuroni\n",
        "n_outputs = 1 # ci serve solo la probabilità che il carrello acceleri a sinistra\n",
        "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
        "\n",
        "# 2. Costruzione della rete neurale\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,n_inputs])\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu,\n",
        "                         kernel_initializer=initializer)\n",
        "logits = tf.layers.dense(hidden, n_outputs, kernel_initializer=initializer)\n",
        "outputs = tf.nn.sigmoid(logits)\n",
        "\n",
        "# 3. Selezione di un'azione casuale basata su quelle stimate (multinomial sampling)\n",
        "\n",
        "prob_left_right = tf.concat(axis=1, values=[outputs, 1-outputs])\n",
        "action = tf.multinomial(tf.log(prob_left_right), num_samples=1)\n",
        "\n",
        "init=tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-10-5df27551b26f>:14: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-10-5df27551b26f>:21: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fojX-nqGaIaw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Analizziamo il codice:\n",
        "\n",
        "1. Dopo import, definiamo l'architettura della nostra rete. Il numero di input è pari alla grandezza dello spazio delle osservazioni (4), utilizziamo 4 hidden neuroni e si ha solo 1 neurone di output che ci da la probabilità di andare a sinistra.\n",
        "\n",
        "2. Costruzione della rete neurale. In questo esempio abbiamo una rete con un singolo output e un solo layer hidden. L'output layer usa la funzione di attivazione sigmoid in modo da dare come output una probabilità compresa tra 0.0 e 1.0. Se ci fossero state più di due azioni possibili nell'ambiente da parte dell'agente, allora ci sarebbe stato un neurone di output per ogni azione, e ovviamente sarebbe stato necessario l'uso della funzione softmax.\n",
        "\n",
        "3. Infine, utilizziamo la funzione *multinomial()* per prelevare un'azione casuale. **Questa funzione campiona in modo del tutto indipendente uno o più numeri interi, data la probabilità di logaritmo di ogni intero**. Ad esempio, se chiamiamo la funzione su un array **[ np.log(0.5), np.log(0.2), np.log(0.3) ]** e **num_samples=5** , si hanno in output **5 interi**, ognuno dei quali avrà una probabilità del **50% di essere 0**, il **20% di essere 1** e il **30% di essere 2**. Nel nostro caso c'è bisogno di un solo intero che rappresenti l'azione da prendere* (c'è solo 1 intero che avrà una probabilità di (prob_left_right) di essere 0 (accelerazione sinistra))*.\n",
        "Poichè il tensore *outputs* contiene solo la probabilità di andare a sinistra, dobbiamo prima concatenare *1-outputs* per avere un tensore che contiene le probabilità di entrambe le azioni, sinistra e destra. **Nota che se ci fossero più di due azioni possibili, la rete neurale avrebbe dovuto produrre una probabilità per(ogni) azione, quindi non avresti avuto bisogno della fase di concatenazione.**\n",
        "\n",
        "Ora abbiamo la nostra policy con rete neurale (NN policy) che prenderà osservazioni e darà come output le azioni. Ma come l'addestriamo?"
      ]
    },
    {
      "metadata": {
        "id": "g_Lp49s1f2Pi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Valutazione delle azioni: Il problema dell'assegnazione del reward (The Credit Assignment Problem)\n",
        "\n",
        " Se sapessimo quale fosse l'azione migliore in ogni fase (step), potremmo addestrare la rete neurale come al solito, minimizzando la cross entropy tra la probabilità stimata (y^) e la probabilità di destinazione (y). Come fosse un apprendimento supervisionato, dove si ha la probabilità target y e quella stimata y^ dalla rete neurale. Tuttavia nell'Apprendimento Rinforzato, l'unica guida che l'agente ha è quella attraverso i rewards e i rewards in genere sono scarsi e arrivano in ritardo. Ad esempio, se l'agente riesce a bilanciare l'asta per 100 step, come può sapere quale di queste 100 azioni è stata buona e quali di queste erano cattive? Tutto quello che sa è che l'asta è caduta dopo l'ultima azione, ma sicuramente quest'ultima azione non è interamente responsabile, ci sarà stato un insieme di azioni che ha eseguito che ha portato alla caduta dell'asta. Questo problema è chiamato *Credit assignment problem*: dove l'agente riceve dei rewards(o ricompensa), ma è difficile per lui determinare quali azioni dovrebbero essere accreditate (o incolpate) per essa (per il reward). In breve, l'agente ha bisogno di sapere quale sequenza di azioni ha portato il carrello a far mantenere l'asta in verticale e quali no, ha bisogno di sapere quali azioni devono essere accreditate per buone e quali no. Si pensi ad un cane che riceve la sua ricompensa dopo ore che si è comportato bene; come può capire per che cosa è stato ricompensato ?\n",
        "Esempio pratico: finchè il carrello non cade esegui l'azione, sta ricevendo sempre rewards positivi, ma ad un tratto, l'asta cade, e  arriva il rewards negativo, come fa l'agente a sapere ora quale sequenza di azioni ha causato la caduta dell'asta? sa solo che nell'ultima azione ha ricevuto un rewards negativo, l'asta è caduta. Potrebbe pensare che tra le 100 azioni che ha eseguito solo l'ultima ha causato la caduta del carello, quindi conviene scartarla, ma anche in questo modo non funzionerebbe, le azioni che esegue sono solo quelle di accelerare verso destra o sinistra, non ha una visione generale sul numero di azioni che deve eseguire per mantenere l'asta in verticale e quindi accreditare queste azioni.\n",
        "\n",
        "C'è bisogno di una strategia che valuti un'azione basandosi sulla somma di tutti i rewards che ne conseguono da essa, applicando un fattore di *discount  γ (gamma)* ad ogni step. Ad esempio (guardare la figura):\n",
        "\n",
        "![alt text](https://i.gyazo.com/c0e247df550c2d9f5e4a4014448fb868.png)\n",
        "\n",
        "Se l'agente decide di andare a destra 3 volte di seguito e riceve come ricompensa +10 dopo il primo step, 0 dopo il secondo e infine -50 dopo il terzo passo, quindi assumendo che si usa un fattore di sconto γ = 0.8, la prima azione avrà un puntaggio totale di 10 + γ x 0 + γ^2 x (-50) = -22. Se il fattore gamma è vicino 0, i rewards futuri non contano molto rispetto ai rewards immediati. Viceversa, se il fattore di sconto è vicino a 1, i rewards futuri (lontani) contano quasi quanto i rewards immediati. Tipicamente il fattore gamma è 0.95 o 0.99. Con gamma pari a 0.95, i rewards di 13 step nel futuro contano all'incirca come la metà dei rewards immediati (0.95^13 ≈ 0.5), mentre con un fattore di 0.99, i rewards di 69 step nel futuro contano come la metà dei rewards immediati.Nell'ambiente del carrrello con asta, le azioni hanno effetti a breve termine, quindi la scelta di un fattore gamma di 0.95 sembra ragionevole. Naturalmente, una buona azione può essere seguita da diverse azioni sbagliate che fanno cadere rapidamente l'asta, con il risultato che la buona azione ottiene un punteggio basso (come quando un buon attore recita in un film terribile).\n",
        "Tuttavia, se effettuiamo abbastanza prove, in media le buone azioni otterranno un punteggio migliore di quelle cattive. Quindi, per ottenere punteggi di azioni affidabili, dobbiamo eseguire molti episodi e normalizzare tutti i punteggi delle azioni (sottraendo la media e dividendo per la deviazione standard, in modo da avere una distribuzione dei punteggi normale). Dopodichè, possiamo presumere che le azioni con punteggi negativi fossero negative, mentre le azioni con un punteggio buono erano buone.\n",
        "\n",
        "Perfetto, ora abbiamo un modo per valutare ogni azione, il passo successivo consiste nel formare un'agente usando i gradienti del policy (*Policy Gradients*)."
      ]
    },
    {
      "metadata": {
        "id": "uGUkt6vse9F7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Policy Gradients\n",
        "\n",
        "Gli algoritmi PG ottimizzano i parametri di una policy seguendo i gradienti verso i rewards più alti. Una classe popolare di algoritmi PG, chiamati *REINFORCE algorithms* è stata introdotta nel 1992 da Ronald Williams. Ecco una variante di questi algoritmi:\n",
        "\n",
        "1. Primo, lasciare che la policy della rete neurale giochi più volte e ad ogni step calcoli i gradienti che renderebbero ancora più probabile l'azione scelta, ma non utilizzare ancora questi gradienti.\n",
        "\n",
        "2. Dopo aver eseguito diversi episodi, si calcola il punteggio di ciascuna azione (utilizzando il metodo descritto nella sezione precedente).\n",
        "\n",
        "3. Se il punteggio di un'azione è positivo, significa che l'azione è stata buona e si applicano i gradienti calcolati prima per quella azione per renderla una scelta più probabile nel futuro. Tuttavia, se il punteggio è negativo, significa che l'azione è stata negativa e si applicano i gradienti opposti per rendere la scelta di questa azione meno probabile nel futuro. La soluzione è semplicemente moltiplicare ogni vettore gradiente per il punteggio dell'azione corrispondente.\n",
        "\n",
        "4. Infine, si calcola la media di tutti i vettori di gradiente risultanti, per utilizzarla nel calcolo dello step per il Gradient Descent.\n",
        "\n",
        "Implementiamo questo algoritmo in TensorFlow. Addestreremo una rete neurale policy costruita in precedenza in modo che impari a bilanciare l'asta sul carrello. Iniziamo a completare la fase di costruzione iniziata in precedenza, aggiungiamo la probabilità di target, la funzione di costo e il training operation. \n",
        "\n",
        "Poichè agiamo come se l'azione scelta fosse l'azione migliore possibile, a probabilità di destinazione deve essere 1 se l'azione scelta è quella di andare a sinistra (0 ) e 0.0 se l'azione scelta è quella di andare a destra (1). Effettuiamo questa definizione perchè non abbiamo labels come nel caso del supervised learning, quindi c'è bisogno del target label per addestrare la rete."
      ]
    },
    {
      "metadata": {
        "id": "ua_gT4e-7LAy",
        "colab_type": "code",
        "outputId": "cbec60f2-3b35-4cb2-adba-7007bf7da87d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "y = 1. - tf.to_float(action)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-8f8dde413370>:1: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MTT4lXnBokUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ora che abbiamo la probabilità di target, definiamo la cost function (cross entropy) e calcolare i gradienti:"
      ]
    },
    {
      "metadata": {
        "id": "m0E9v8eEoL0Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "grads_and_vars = optimizer.compute_gradients(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-lyqUCAXpoeT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Nota che è stato chiamato il metodo *compute_gradients()* invece del metodo *minimize()*. Questo perchè vogliamo modificare i gradienti prima di applicarli. Il metodo *compute_gradients()* restituisce una lista di coppie vettore gradiente/variabile (una coppia per variabile trainabile). Mettiamo i gradienti in una lista, in modo da ottenerli più facilmente:"
      ]
    },
    {
      "metadata": {
        "id": "d5zws1eZpml_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gradients = [grad for grad, variable in grads_and_vars]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XjL4GPobRN2m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ora arriva la parte difficile. Durante la fase di esecuzione, l'algoritmo eseguirà la policy e ad ogni step valuterà questi tensori di gradiente e memorizzerà i loro valori. Dopo un certo numero di episodi, modificherà i gradienti (moltiplicandoli per i punteggi delle azioni e nornalizzandoli) e calcolando la media dei gradienti modificati. Successivamente sarà necessario riportare i gradienti risultanti all'optimization in modo che si possa eseguire un optimization step. Questo significa che abbiamo bisogno di un placeholder per ogni vettore del gradiente. Inoltre, dobbiamo creare l'operation che applicherà i gradienti aggiornati. Useremo la funzione *apply_gradients()* dell'optimizer, che prende una lista della coppia vettore gradiente/variabile.  Invece, di dargli i vettori del gradiente originale, gli daremo una lista contenente i gradienti aggiornati (cioè quelli del placeholder):"
      ]
    },
    {
      "metadata": {
        "id": "Y_Jv8SKgq6eX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gradient_placeholders = []\n",
        "grads_and_vars_feed = []\n",
        "\n",
        "for grad,variable in grads_and_vars:\n",
        "  gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
        "  gradient_placeholders.append(gradient_placeholder)\n",
        "  grads_and_vars_feed.append((gradient_placeholder, variable))\n",
        "  \n",
        "  \n",
        "training_op = optimizer.apply_gradients(grads_and_vars_feed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I0kirycEWhoW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fase di costruzione completa:"
      ]
    },
    {
      "metadata": {
        "id": "ca2rC1PCUXII",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 1. Specifica dell'architettura della rete neurale\n",
        "\n",
        "n_inputs = 4 # è la dimensione del vettore delle obs env.observation_space.shape[0]\n",
        "n_hidden = 4 # è un task semplice non c'è bisogno di più neuroni\n",
        "n_outputs = 1 # ci serve solo la probabilità che il carrello acceleri a sinistra\n",
        "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
        "\n",
        "# 2. Costruzione della rete neurale\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,n_inputs])\n",
        "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu,\n",
        "                         kernel_initializer=initializer)\n",
        "logits = tf.layers.dense(hidden, n_outputs, kernel_initializer=initializer)\n",
        "outputs = tf.nn.sigmoid(logits)\n",
        "\n",
        "# 3. Selezione di un'azione casuale basata su quelle stimate (multinomial sampling)\n",
        "\n",
        "prob_left_right = tf.concat(axis=1, values=[outputs, 1-outputs])\n",
        "action = tf.multinomial(tf.log(prob_left_right), num_samples=1)\n",
        "\n",
        "# 4. Definizione del label target e cost function\n",
        "y = 1. - tf.to_float(action)\n",
        "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, \n",
        "                                                        logits=logits)\n",
        "learning_rate = 0.01\n",
        "# 5. Definizione del PG algorithm\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
        "gradients = [grad for grad, variable in grads_and_vars]\n",
        "gradient_placeholders = []\n",
        "grads_and_vars_feed = []\n",
        "\n",
        "for grad,variable in grads_and_vars:\n",
        "  gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
        "  gradient_placeholders.append(gradient_placeholder)\n",
        "  grads_and_vars_feed.append((gradient_placeholder, variable))\n",
        "  \n",
        "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zkJnC4hiYne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Prima della fase di esecuzione, bisogna definire due funzioni, per calcolare la somma dei rewards dati i rewards e normalizzarli durante gli episodi:\n",
        "\n",
        "![alt text](https://cloud.githubusercontent.com/assets/2981167/23574559/6195a456-0034-11e7-8a71-c25092869edf.png)\n",
        "\n",
        "![alt text](https://cloud.githubusercontent.com/assets/2981167/23574363/80fb2836-0032-11e7-8a91-dcbf21c3d0e5.png)"
      ]
    },
    {
      "metadata": {
        "id": "omD63MVEXPDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d25d75e7-e27a-40f6-ce42-4b3306e580ad"
      },
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, gamma):\n",
        "  discount_rewards = np.empty(len(rewards))\n",
        "  rewards_acc = 0\n",
        "  for step in reversed(range(len(rewards))):\n",
        "    rewards_acc = rewards[step] + rewards_acc * gamma\n",
        "    discount_rewards[step] = rewards_acc\n",
        "  return discount_rewards\n",
        "\n",
        "\n",
        "\n",
        "discount_rewards([10,0,-50], 0.8)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-22., -40., -50.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "BqjSI8oxlZAn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}