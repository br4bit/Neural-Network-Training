{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Guide Reinforcement Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/br4bit/Neural-Network-Training/blob/master/Guide_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7y5M_RAVXbne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning (Apprendimento Rinforzato)\n",
        "\n",
        "Il *Reinforcement Learning (RL)* è uno dei campi più eccitanti del Machine Learning oggi, e anche uno dei più vecchi, esiste dagli anni '50.\n",
        "\n",
        "## Imparare ad ottimizzare i rewards.\n",
        "\n",
        "Nel Reinforcement Learning, un software *agent* effettua delle *osservazioni* e intraprende delle *azioni* all'interno di un *ambiente (environment)*  e in cambio riceve dei *rewards*.\n",
        "\n",
        "Il suo obiettivo è imparare ad agire in modo da massimizzare i suoi *rewards* a lungo termine. E' possibile pensare a *rewards* positivi come un piacere, e *rewards* negativi come un dolore/pena. In breve, l'agente agisce nell'ambiente e impara per tentativi ed errori a massimizzare il suo piacere e a minimizzare la sua pena.\n",
        "\n",
        "Vediamo alcuni esempi di applicazione:\n",
        "\n",
        "1 (a). L'agente può essere un programma che controlla il movimento di un robot. In questo caso, l'ambiente è il mondo reale, l'agente osserva l'ambiente attraverso un set di sensori, come camera e sensori touch, e agisce inviando segnali ai motori per attivarli. Può essere programmato per ricevere rewards positivi ogni volta che si avvicina alla destinazione, e reward negativi qualora perdesse tempo, andando nella direzione sbagliata o cade.\n",
        "\n",
        "2 (b). L'agente può essere un programma che controlla Ms. Pac-Man. In questo caso, l'ambiente è la simulazione del gioco Atari. le azioni da intraprendere sono le 9 possibili posizioni del joystick (sopra,sotto,e così via), le osservazioni da cui osservare per intraprendere le azioni saranno gli screenshots, mentre i rewards i game point.\n",
        "\n",
        "3 (c). L'agente può essere un semplice programma che gioca ad un gioco da tavolo come *Go*.\n",
        "\n",
        "4 (d). Non necessariamente l'agente deve controllare dei movimenti virtuali o reali di un qualcosa. Ad esempio, può essere un semplice termostato, che riceve dei rewards ogni volta che è vicino alla suo obiettivo di temperatura e risparmia energia e rewards negativi ogni volta che una mano esterna gli modifichi la temperatura, in questo modo l'agente impara ad anticipare i bisogni umani.\n",
        "\n",
        "5 (e). L'agente può osservare il prezzo di una merce in magazzino per decidere quando comprare e vendere ogni secondo. I rewards saranno ovviamente i guadagni e le perdite monetarie.\n",
        "\n",
        "![alt text](https://i.gyazo.com/thumb/1000/8e9980121e73e72575469bc49abaff56-png.jpg)\n",
        "\n",
        "\n",
        "Da notare che non sempre possono esserci rewards positivi; per sempio, l'agente può muoversi in un labirinto, ricevendo rewards negativi ad ogni istante di tempo, dove è meglio trovare un'uscita il più velocemente possibile. Ci sono un mucchio di task dove il RL è stato applicato come nel self-driving cars, per gli ads di una web page, o per controllare dove l'attenzione è focalizzata in un sistema di classificazione d'immagini."
      ]
    },
    {
      "metadata": {
        "id": "QX4yG267zmZY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Policy Search (Determinazone delle azioni da intraprendere)\n",
        "\n",
        "Il *Policy Search* può essere visto come una linea di condotta o un metodo per determinare le azioni che l'agente deve intraprendere. In breve, è un algoritmo usato dal software agent per determinare le sue azioni, è chiamato la sua *policy*. Per esempio, la policy potrebbe essere una rete neurale che riceve in input le osservazioni e da come output le azioni da compiere:\n",
        "\n",
        "![alt text](https://i.gyazo.com/d2553cf9477314cc2d9db6282b49562c.png)\n",
        "\n",
        "\n",
        "La policy può essere qualsiasi algoritmo a cui si possa pensare, e non deve essere nemmeno deterministico. Ad esempio, consideriamo un'aspirapolvere robotica, la cui ricompensa è la quantità di polvere che raccoglie in 30 minuti. La sua policy potrebbe essere quella di muoversi in avanti con una probabilità *p* ogni secondo, oppure ruotare in modo casuale a destra o a sinistra con una probabilità di *1-p*. L'angolo di rotazione sarà un angolo casuale compreso tra *-r e r*. Dal momento che questa policy introduce un pò di casualità è chiamata *stochastic policy*.\n",
        "Il robot avrà una traiettoria errata, che garantisce che alla fine raggiungerà qualsiasi posto possa raggiungere e raccoglierà tutta la polvere. La domanda è: Quanta polvere riuscirà a raccogliere in 30 minuti?\n",
        "\n",
        "Come si può addestrare questo robot? Ci sono solo due *policy parametri* che si possono modificare: la probabilità *p* e l'angolo *r*. Un possibile algoritmo di apprendimento potrebbe essere quello di provare molti valori diversi per questi parametri, e scegliere la combinazione che performa al meglio. Questo è un esempio di *policy search*, in questo caso utilizzando un approccio a forza bruta. Tuttavia quando lo *spazio di policy* è molto ampio, trovare un buon set di parametri in questo modo è come cercare un ago in un pagliaio.\n",
        "\n",
        "Un'altra strada per esplorare lo *spazio di policy (policy space)*  è di usare un *genetic algorithms (algoritmi genetici)*. Per esempio, si potrebbe creare casualmente una prima generazione di 100 policies (criteri) e provarli, uccidere le 80 peggiori policies, e lasciare le restanti 20 come sopravvisuti che producono 4 figli ciascuno. Un figlio non è altro che una copia del suo genitore più una variazione casuale. Le policies sopravvisuti più la loro prole (figli) insieme costituiranno la seconda generazione. Il processo si itera attraverso tutte le generazioni in questo modo, finchè non si trova la policy perfetta.\n",
        "\n",
        "![alt text](https://i.gyazo.com/ff957f4cf89c999079ebacbcb494c83c.png)\n",
        "\n",
        "\n",
        "Un altro approccio può essere quello di una tecnica di ottimizzazione, che valuta i gradienti di ogni reward rispetto ai parametri della policy, quindi aggiustando questi parametri seguendo il gradiente verso i rewards più alti *gradient ascent*. Questo approccio è chiamato *policy gradients (PG)*.\n",
        "Per esempio, torniamo indietro all'aspirapolvere, si potrebbe aumentare leggermente *p* e valutare se questo aumenta la quantità di polvere raccolta dal robot in 30 minuti; se lo fa, allora incrementa *p* un pò di più, o altrimenti riduci *p*. \n",
        "\n",
        "Prima di implementare l'algoritmo di PG usando TensorFlow, c'è bisogno di creare un ambiente per l'agente dove possa vivere. Introduciamo OpenAI gym."
      ]
    },
    {
      "metadata": {
        "id": "qYZQv-Fm9Jz7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduzione OpenAI gym\n",
        "\n",
        "Una delle sfide dell'apprendimento rinforzato è che per addestare un agente c'è bisogno di un ambiente di lavoro. Se si vuole programmare un agente che impara a giocare ad un gioco Atari, c'è bisogno di un simulatore di gioco Atari. Se si vuole programmare un robot che cammini, allora l'ambiente è il mondo reale dove si può addestrare direttamente il robot in quel ambiente, ma con delle limitazioni: se il robot cade da una scogliera, non basta solo \"annullare\" l'azione. Non è possibile accelerare il tempo; aggiungere più potenza computazionale non farà si che il robot si muova più velocemente. E in genere è molto costoso addestrare 1000 robot in parallelo.\n",
        "\n",
        "In breve, l'addestramento nel mondo reale è lento e difficile, quindi c'è bisogno di un simulatore di ambienti, almeno per la prima fase dell'addestramento (bootstrap training).\n",
        "\n",
        "*OpenAI gym* è un kit di strumenti che forniscono una varietà di simulatori di ambienti (Atari, tavola degli scacchi, mondo 2D, 3D, ecc), così si possono addestrare agenti, compararli tra di loro e implementare nuovi algoritmi di RL.\n",
        "\n",
        "Installazione di *OpenAI gym*:"
      ]
    },
    {
      "metadata": {
        "id": "2wTFk6WCT20x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QN3c2HmRIRXN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creazione del primo ambiente:"
      ]
    },
    {
      "metadata": {
        "id": "GmKAr3woLJ2W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In google colab per utilizzare la funzione env.render() c'è bisogno di un piccolo passaggio:"
      ]
    },
    {
      "metadata": {
        "id": "C05DB6fSLRnF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OcGkLYYLddT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0d892a46-5b24-426f-c7d6-097ee7ee7a01"
      },
      "cell_type": "code",
      "source": [
        "#eg screen resolution 1400x900\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1009'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1009'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "2QjXbY2ILren",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gpAtOTnpIKhW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "env = gym.make(\"CartPole-v0\") #creazione ambiente\n",
        "\n",
        "obs = env.reset() #osservazioni\n",
        "\n",
        "obs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D2yCq6U4OL0Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "La funzione *make()* crea l'ambiente, in questo caso il carrello con asta. E' una simulazione 2D dove il carrello può accelerare nella direazione di destra e sinistra, per bilanciare l'asta posta al di sopra di esso:\n",
        "\n",
        "![alt text](https://i.gyazo.com/0a9d7b15a9877b4ab5351ac643414084.png)\n",
        "\n",
        "Dopo che l'ambiente è stato creato, bisogna inizializzarlo usando il metodo *reset()*. Il metodo ritorna la prima osservazione. Le osservazioni dipendono dal tipo di ambiente. Per il carrello con asta, ogni osservazione è un NumPy array di 1D, contenente 4 valori di tipo float: questi valori rappresentano, la posizione del carrello in orizzontale (0.0 = centro), la sua velocità, l'angolo dell'asta (0.0 = verticale) e la sua velocità angolare.\n",
        "\n",
        "Chiediamo ora all'ambiente quali sono le possibili azioni con il metodo *action_space*:"
      ]
    },
    {
      "metadata": {
        "id": "O6b3U9PhIfcg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W4tF4ytzRgyC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Discrete(2)* significa che le possibili azioni sono interi 0 e 1, che rappresentano l'accelerazione vero destra (1) e verso sinistra (0). Altri ambienti avranno più azioni o un altro tipo di azioni (nel continuo).\n",
        "Dato che l'asta è inclinata verso destra, acceleriamo il carrello verso destra:"
      ]
    },
    {
      "metadata": {
        "id": "2orD8WzNQBSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47f2aebf-f352-474f-8d6d-d79a169de292"
      },
      "cell_type": "code",
      "source": [
        "action = 1 #accelerazione verso destra\n",
        "\n",
        "obs, reward, done, info = env.step(action)\n",
        "\n",
        "print(obs,reward,done,info)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.07195535  0.55893402 -0.06550182 -0.94555214] 1.0 False {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iTlUTkgAWFLx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "SlnF4GKnUifJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}